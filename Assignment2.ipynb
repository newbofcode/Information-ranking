{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_lower(my_list): #from https://blog.finxter.com/python-convert-string-list-to-lowercase/#:~:text=The%20most%20Pythonic%20way%20to,new%20string%20list%2C%20all%20lowercase.\n",
    "    return [x.lower() for x in my_list]\n",
    "def removeField(discarded_fields,file_path):\n",
    "    # Ask the user for the output file name\n",
    "    output_file_path = f\"{file_path}{discarded_fields}FieldRemoved.txt\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file, open(output_file_path, 'w') as output_file:\n",
    "            xStop = False\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                line2 = line.split()\n",
    "                if line2[0] in discarded_fields:\n",
    "                    xStop = True\n",
    "                if \".I\" in line2:\n",
    "                    xStop = False\n",
    "                if not xStop:\n",
    "                    output_file.write(line + '\\n')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    return output_file_path\n",
    "\n",
    "def stopwords(stopwordfile):\n",
    "    #read stopwords.txt\n",
    "    try:\n",
    "        with open(stopwordfile,'r') as wordfile:\n",
    "            stopwords_set = all_lower(list(wordfile.read().splitlines()))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {stopwordfile} does not exist.\")\n",
    "        exit(1)\n",
    "    return stopwords_set\n",
    "    \n",
    "def dictionaryCreate(wordList, dictionaryFile,currentID,document_list = []):\n",
    "    already_added = False\n",
    "    for word in wordList:\n",
    "        word = word.lower()\n",
    "        if word in document_list:\n",
    "            already_added = True\n",
    "        else:\n",
    "            document_list.append(word)\n",
    "            already_added = False\n",
    "        if already_added == False:\n",
    "            if word in dictionaryFile:\n",
    "                dictionaryFile[word] += 1\n",
    "            else:\n",
    "                dictionaryFile[word] = 1\n",
    "    return dictionaryFile, document_list\n",
    "def postingPosList(wordList, currentDocId, filePath,line_number, line,posting_frequency, completed_article = bool, old_posting_frequency ={}, new_posting_frequency ={}):\n",
    "    wordOccurrences = posting_frequency \n",
    "    postings_list = {}\n",
    "    wordList = {k.lower(): v for k, v in wordList.items()}\n",
    "    if completed_article and not old_posting_frequency:\n",
    "        for word in posting_frequency.keys():\n",
    "            word = word.lower()\n",
    "            the_set = [(currentDocId,wordList[word],posting_frequency[word])]\n",
    "            \n",
    "            if word in postings_list:\n",
    "                postings_list[word].extend(the_set)\n",
    "            else:\n",
    "                postings_list[word] = the_set\n",
    "        return postings_list\n",
    "    if completed_article and old_posting_frequency:\n",
    "        for word in posting_frequency.keys():\n",
    "            if word in old_posting_frequency:\n",
    "                old_posting_frequency[word].extend(new_posting_frequency[word])\n",
    "            else:\n",
    "                old_posting_frequency[word] = new_posting_frequency[word]\n",
    "        #print(old_posting_frequency)\n",
    "        return old_posting_frequency\n",
    "                \n",
    "    for word in wordList.keys():\n",
    "        word = word.lower()\n",
    "        pattern = re.compile(rf'\\b{re.escape(word)}\\b', re.IGNORECASE)\n",
    "        matches = pattern.finditer(str(line))\n",
    "        positions = [(line_number,match.start()+1) for match in matches]\n",
    "        word = str(word)\n",
    "        if positions:\n",
    "            if word in wordOccurrences:\n",
    "                wordOccurrences[word].extend(positions)\n",
    "            else:\n",
    "                wordOccurrences[word] = positions\n",
    "    return wordOccurrences\n",
    "      \n",
    "stemmingOn = False\n",
    "stopwordsOn = False     \n",
    "def invert():\n",
    "    # The variables\n",
    "    global stemmingOn, stopwordsOn\n",
    "    file_path = \"cacm.all\"\n",
    "    #file_path_old = file_path\n",
    "    #file_path = removeField(['.B','.A','.N','.X'],file_path)\n",
    "    stopwords_file_path = \"stopwords.txt\"\n",
    "    stopwords_set = stopwords(stopwords_file_path)\n",
    "    try:\n",
    "        stemmingON = input(\"Do you wish to turn stemming on?(Press 1 for yes, anything else for no): \")\n",
    "        stopwordsON = input(\"Do you wish to remove stopwords?(Press 1 for yes, anything else for no): \")\n",
    "        if stemmingON == '1':\n",
    "            stemmingOn = True\n",
    "        if stopwordsON == '1':\n",
    "            stopwordsOn = True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "    try: \n",
    "        documentID_counter=[]\n",
    "        documentID = 0\n",
    "        startIndexing = False\n",
    "        dictionaryFile = {}\n",
    "        postingFile = {}\n",
    "        postingFreq ={}\n",
    "        old_postingFile = {}\n",
    "        line_counter = 0\n",
    "        line_reset_at_new_id = 0\n",
    "        #examplecounter = 0\n",
    "        with open(file_path, 'r') as file:\n",
    "            #text = file.read()\n",
    "            words = []\n",
    "            for line in file:\n",
    "                line_counter +=1\n",
    "                line_reset_at_new_id +=1\n",
    "                line = line.strip()\n",
    "                if \".I\" in line:\n",
    "                    line_reset_at_new_id=0\n",
    "                    startIndexing = False\n",
    "                    if line_counter >= 2:\n",
    "                        \n",
    "                        postingFile = postingPosList(word_frequencies, documentID, file_path, line_reset_at_new_id-1,line.lower(),postingFreq,True)\n",
    "                        if not old_postingFile:\n",
    "                            old_postingFile = dict(postingFile)\n",
    "                        else:\n",
    "                            postingFile = postingPosList(word_frequencies, documentID, file_path, line_reset_at_new_id-1,line.lower(),postingFreq,True, old_postingFile, postingFile)\n",
    "                            \n",
    "                    documentID = line.split()[1]\n",
    "                    document_counter = []\n",
    "                    words =[]\n",
    "                    postingFreq ={}\n",
    "                if \".T\" in line:\n",
    "                    startIndexing = True\n",
    "                if startIndexing:\n",
    "                    line = re.sub(r'(?<![0-9])\\W+(?![0-9])', ' ', line)\n",
    "                    line = re.sub(r'[()]', '', line)\n",
    "                    line = re.sub(r'(\\d)([a-zA-Z])', r'\\1 \\2', line)\n",
    "                    #line = re.sub(r'[\\'\"]', '', line)\n",
    "                    if stemmingOn:#from a site\n",
    "                        stemmer = PorterStemmer()\n",
    "                        linewords = nltk.word_tokenize(line)\n",
    "                        stemmed_words = [stemmer.stem(word) for word in linewords]\n",
    "                        line = ' '.join(stemmed_words)  \n",
    "                    word = line.split()\n",
    "                    words.extend(word)\n",
    "                    if stopwordsOn == True:\n",
    "                        filtered_words = [word for word in words if word.lower() not in stopwords_set]\n",
    "                        words = filtered_words\n",
    "                    word_frequencies = Counter(words)\n",
    "                    postingFreq = postingPosList(word_frequencies, documentID, file_path, line_reset_at_new_id-1,line.lower(),postingFreq,False)\n",
    "                    dictionaryFile, document_counter = dictionaryCreate((word_frequencies.keys()), dictionaryFile,documentID,document_counter)\n",
    "                    \n",
    "                    #examplecounter +=1\n",
    "                    #if examplecounter == 500:\n",
    "                        #print(\"exit\")\n",
    "                        #exit(1)\n",
    "        posting_file_name = \"PostingsFile.txt\"\n",
    "        dictionary_file_name = \"dictionaryFile.txt\"\n",
    "        json.dump(sorted(dictionaryFile.items()), open(\"dictionaryFile.txt\",'w'))\n",
    "        json.dump(sorted(postingFile.items()), open(\"PostingsFile.txt\",'w'))\n",
    "        return [file_path,dictionary_file_name,posting_file_name,documentID]\n",
    "        \"\"\"\n",
    "        with open('dictionaryFile.txt','w') as dictionary_file_output, open('PostingsFile.txt','w') as postings_file_output:\n",
    "            dictionary_file_output.write(\"(Term,Document Appearances)\"+ '\\n')\n",
    "            postings_file_output.write(\"(Term, [(DocumentID, Frequency in Document, [(Row#, Column#),*Other occurrences*]),*Other Documents*])\"+ '\\n')\n",
    "        with open('dictionaryFile.txt','a') as dictionary_file_output, open('PostingsFile.txt','a') as postings_file_output:\n",
    "            for i in sorted(dictionaryFile.items()):\n",
    "                dictionary_file_output.write(str(i) + '\\n') \n",
    "            for i in sorted(postingFile.items()):\n",
    "                postings_file_output.write(str(i) + '\\n')\n",
    "        \"\"\"\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(files,query,k=10,evaluation=False):\n",
    "    dictionary_file = files[1]\n",
    "    N = int(files[3])\n",
    "    posting_file = files[2]\n",
    "    user_query = sorted(query[1].items())#sets where (term,freq)\n",
    "    query_weights = [0]*len(user_query)\n",
    "    doc_weights_dictionary = {}\n",
    "    try:\n",
    "        dictionary_doc= dict(json.load(open(dictionary_file)))\n",
    "        posting_doc = dict(json.load(open(posting_file)))\n",
    "        #print(\"check 1\")\n",
    "        #start IDF calculations\n",
    "        idf_scores = {}\n",
    "        for term in dictionary_doc.keys():\n",
    "            df = int(dictionary_doc[term])\n",
    "            idf_scores[term] = math.log(N/df,10)\n",
    "        #json.dump(idf_scores, open(\"idf_scores.txt\",'w'))\n",
    "        # end IDF calculations\n",
    "        #print(\"check 2\")\n",
    "        # find documents\n",
    "        #print(user_query)\n",
    "        for index, term_freq in enumerate(user_query):\n",
    "            #print(term_freq)\n",
    "            \n",
    "            \n",
    "            #print(f\"check 3, {term_freq}\")\n",
    "            if term_freq[0] in dictionary_doc:\n",
    "                #query_weights[index] = (1 + math.log(term_freq[1],10))*(idf_scores[term_freq[0]])\n",
    "                if idf_scores[term_freq[0]] >= 0.6:\n",
    "                    query_weights[index] = (1 + math.log(term_freq[1],10))*(idf_scores[term_freq[0]])\n",
    "                else:\n",
    "                    query_weights[index] = (1 + math.log(term_freq[1],10))*(0)\n",
    "                for docID in posting_doc[term_freq[0]]:\n",
    "                    doc_id = docID[0]\n",
    "                    word_freq = int(docID[1])\n",
    "                    tf = 1 + math.log(word_freq,10)\n",
    "                    idf = idf_scores[term_freq[0]]\n",
    "                    #if idf < 1.0:\n",
    "                        #idf = 0\n",
    "                    w = tf * idf\n",
    "                    if doc_id in doc_weights_dictionary:\n",
    "                        doc_weights_dictionary[doc_id][index] = w\n",
    "                    else:\n",
    "                        doc_weights_dictionary[doc_id] = [0]*len(user_query)\n",
    "                        doc_weights_dictionary[doc_id][index] = w\n",
    "        sim_score_dictionary = simScore(doc_weights_dictionary,query_weights)\n",
    "        sorted_sim_score_dictionary = sorted(sim_score_dictionary.items(), key=lambda item: (-item[1], item[0]))\n",
    "        #print(query_weights)\n",
    "        #print(query_weights)\n",
    "        #print(doc_weights_dictionary['1350'])\n",
    "        #print(doc_weights_dictionary['1134'])\n",
    "        #print(sorted_sim_score_dictionary)\n",
    "        if len(sorted_sim_score_dictionary)>9:\n",
    "            top_k = sorted_sim_score_dictionary[:k]\n",
    "        else:\n",
    "            top_k = sorted_sim_score_dictionary\n",
    "        #print(top_k)\n",
    "        top_k_dic_id = [doc[0] for doc in top_k]\n",
    "        #print(top_k_dic_id)\n",
    "        original_doc = files[0]\n",
    "\n",
    "        begin_copy = False\n",
    "        with open(original_doc,'r') as f:\n",
    "            counter=0\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                if \".I\" in line:  \n",
    "                    if line[1] in top_k_dic_id:\n",
    "                        index = top_k_dic_id.index(line[1])\n",
    "                        begin_copy = True\n",
    "                        title = []\n",
    "                        author = []\n",
    "                        author_section = False\n",
    "                        title_section = False\n",
    "                    else:\n",
    "                        begin_copy = False\n",
    "                        author_section = False\n",
    "                        title_section = False\n",
    "                        continue\n",
    "                if any(element == \".B\" or element == \".W\" or element == \".N\" or element == \".X\" or element == \".K\" or element == \".C\"for element in line):\n",
    "                    title_section = False\n",
    "                    author_section = False\n",
    "                if \".T\" in line and begin_copy:\n",
    "                    title_section = True\n",
    "                    author_section = False\n",
    "                    continue \n",
    "                if \".A\" in line and begin_copy:\n",
    "                    author_section = True\n",
    "                    title_section = False\n",
    "                    continue\n",
    "                if begin_copy and title_section:\n",
    "                    title.extend(line)\n",
    "                    continue\n",
    "                if author_section and begin_copy:\n",
    "                    if len(author) > 1:\n",
    "                        author.append(\",\")\n",
    "                    author.extend(line)\n",
    "                        \n",
    "                    continue\n",
    "                if \".X\" in line and begin_copy:\n",
    "                    top_k_dic_id[index] = \" \".join(title) + \" by: \" + \" \".join(author)  \n",
    "        #print(top_k_dic_id)       \n",
    "        json.dump(top_k, open(\"TopK.txt\",'w')) #[[documentID, relevance scores]] \n",
    "        if evaluation == False:   \n",
    "            for rank,items in enumerate(top_k_dic_id):\n",
    "                print(f\"___________________________________________________________________________\\nRank {rank+1}:\\n{items}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{files[0]}' or '{files[1]}' or '{files[2]}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "    \n",
    "    return top_k\n",
    "\n",
    "def simScore(doc_weights, query_weights):\n",
    "    sqrt_weights = {}\n",
    "    sim_score_dict = {}\n",
    "\n",
    "    query_weights_squared = [num ** 2 for num in query_weights]\n",
    "\n",
    "    sqrt_weights[\"Q\"] = math.sqrt(sum(query_weights_squared))\n",
    "\n",
    "    for index,weight in enumerate(doc_weights):\n",
    "        squared_weights = [num ** 2 for num in doc_weights[weight]]#square each element\n",
    "        #squared_weights = [num * 0 if num < 0.3 else num ** 2 for num in doc_weights[weight]]\n",
    "        sum_of_weights = sum(squared_weights)#sum of entire list\n",
    "        sqrt_weights[weight] = math.sqrt(sum_of_weights)#square root of sum\n",
    "        #calculate simScore\n",
    "        result = [a * b for a, b in zip(doc_weights[weight], query_weights)]#produces a list of final results after multiplying element by element from 1 list to another\n",
    "        if sqrt_weights[\"Q\"] == 0:\n",
    "            continue\n",
    "        sim_score_dict[weight] = sum(result)/(sqrt_weights[weight]*sqrt_weights[\"Q\"])\n",
    "    \n",
    "    #print(\"cleared simscore\")\n",
    "    return sim_score_dict\n",
    "def userQuery():\n",
    "    list_of_eclipsed_time = []\n",
    "    global stemmingOn, stopwordsOn\n",
    "    \n",
    "    #ask for query\n",
    "    while True:\n",
    "        user_input = input(\"Please enter the query you wish to search for (Enter ZZEND to end program): \").lower().strip()\n",
    "        if len(user_input) < 1:\n",
    "            continue\n",
    "        user_input = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', user_input)\n",
    "        user_input = re.sub(r'[!@#$%^&*()+\\-_=\\[\\]{}|;:\"\\'<>,.?/\\\\`~]', '', user_input)\n",
    "        if user_input == 'zzend':\n",
    "            return False\n",
    "        user_input = user_input.split()\n",
    "        #print(user_input)\n",
    "        if stopwordsOn:\n",
    "            stopwords_set = stopwords(\"stopwords.txt\")\n",
    "            user_input = [word for word in user_input if word.lower() not in stopwords_set]\n",
    "            user_input = \" \".join(user_input)\n",
    "        if len(user_input) == 0:\n",
    "            continue\n",
    "        user_input_non_stemmed=user_input\n",
    "        if stemmingOn:\n",
    "            stemmer = PorterStemmer()\n",
    "            linewords = nltk.word_tokenize(user_input)\n",
    "            user_input = [stemmer.stem(word) for word in linewords]\n",
    "            user_input = ' '.join(user_input)\n",
    "            user_input = user_input.strip()\n",
    "        if stemmingOn or stopwordsOn:\n",
    "            user_input = user_input.split()\n",
    "        \n",
    "        return [stemmingOn,Counter(user_input),Counter(user_input_non_stemmed),user_input_non_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userQueryEval(input):\n",
    "    list_of_eclipsed_time = []\n",
    "    global stemmingOn, stopwordsOn\n",
    "    \n",
    "    #ask for query\n",
    "    user_input = input\n",
    "    user_input = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', user_input)\n",
    "    user_input = re.sub(r'[!@#$%^&*()+\\-_=\\[\\]{}|;:\"\\'<>,.?/\\\\`~]', '', user_input)\n",
    "    if len(user_input) < 1:\n",
    "            return False\n",
    "    user_input = user_input.split()\n",
    "    #print(user_input)\n",
    "    if stopwordsOn:\n",
    "        stopwords_set = stopwords(\"stopwords.txt\")\n",
    "        user_input = [word for word in user_input if word.lower() not in stopwords_set]\n",
    "        user_input = \" \".join(user_input)\n",
    "    if len(user_input) < 1:\n",
    "            False\n",
    "    user_input_non_stemmed=user_input\n",
    "    if stemmingOn:\n",
    "        stemmer = PorterStemmer()\n",
    "        linewords = nltk.word_tokenize(user_input)\n",
    "        user_input = [stemmer.stem(word) for word in linewords]\n",
    "        user_input = ' '.join(user_input)\n",
    "        user_input = user_input.strip()\n",
    "    if stemmingOn or stopwordsOn:\n",
    "        user_input = user_input.split()\n",
    "    \n",
    "    return [stemmingOn,Counter(user_input),Counter(user_input_non_stemmed),user_input_non_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagePrecision(rel, ret, sumAP):\n",
    "    pList = []\n",
    "    counter=0\n",
    "    for index,id in enumerate(ret):\n",
    "        if id in rel:\n",
    "            counter += 1\n",
    "            p = counter/(index+1)\n",
    "            pList.append(p)\n",
    "    #print(len(rel), pList)\n",
    "    \n",
    "    sumAP+=sum(pList)/len(rel)\n",
    "    \n",
    "    return sumAP\n",
    "def rPrecision(rel, ret, sumRP):\n",
    "    counter = 0\n",
    "    for index,id in enumerate(ret):\n",
    "        if id in rel:\n",
    "            counter += 1\n",
    "        if index + 1 == len(rel):\n",
    "            break\n",
    "    sumRP += counter/len(rel)\n",
    "    \n",
    "    return sumRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(files):\n",
    "    RET_DOC = 'query.text'\n",
    "    REL_DOC = 'qrels.text'\n",
    "    rel_dict = {}\n",
    "    ret_list = {}\n",
    "    file_names_list = files\n",
    "    with open(REL_DOC, 'r') as REL_FILE:\n",
    "        for line in REL_FILE:\n",
    "            line = line.strip().split()\n",
    "            #print(line)\n",
    "            if int(line[0]) in rel_dict:\n",
    "                rel_dict[int(line[0])].append(line[1])\n",
    "            else:\n",
    "                rel_dict[int(line[0])] = [line[1]]\n",
    "        #print(len(rel_dict))\n",
    "    with open(RET_DOC, 'r') as RET_FILE:\n",
    "        counter = 0\n",
    "        start_recording = False\n",
    "        query_eval = \"\"\n",
    "        sumAp = 0\n",
    "        sumRp = 0\n",
    "        for line in RET_FILE:\n",
    "            line = line.strip().split()\n",
    "            if \".I\" in line:\n",
    "                doc_id = int(line[1])\n",
    "                continue\n",
    "            if \".W\" in line:\n",
    "                #counter+=1\n",
    "                start_recording = True\n",
    "                query_eval = \"\"\n",
    "                continue\n",
    "            if start_recording:\n",
    "                query_eval += ' '.join(line) + \" \"\n",
    "            if \".A\" in line:\n",
    "                start_recording = False\n",
    "            if \".N\" in line:\n",
    "                start_recording = False\n",
    "                query_eval = query_eval.strip()\n",
    "                query = userQueryEval(query_eval)\n",
    "                if query == False:\n",
    "                    continue\n",
    "                #clear_output(wait=True)\n",
    "                #k = len(rel_dict[doc_id])\n",
    "                ret_list = search(file_names_list, query,10, True)\n",
    "                ret_list = [id[0] for id in ret_list]\n",
    "                #print(ret_list)\n",
    "                #print(rel_dict[doc_id])\n",
    "                if doc_id in rel_dict:\n",
    "                    sumAp = averagePrecision(rel_dict[doc_id],ret_list,sumAp)\n",
    "                    #print(apList)\n",
    "                    sumRp = rPrecision(rel_dict[doc_id],ret_list,sumRp)\n",
    "                    counter+=1\n",
    "                    #print(len(rel_dict[doc_id]))\n",
    "                \n",
    "            #if counter == 5:\n",
    "                #break\n",
    "        #print(counter)\n",
    "        MAP = sumAp/counter\n",
    "        RPRECISION = sumRp/counter\n",
    "        print(f\"MAP = {MAP}, Average R-Precision = {RPRECISION}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP = 0.1828688572827038, Average R-Precision = 0.20947708484453706\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_names_list = invert()\n",
    "    #file_names_list = ['cacm.all', 'dictionaryFile.txt', 'PostingsFile.txt', '3204']\n",
    "    while True:\n",
    "        query = userQuery() #returns a list of [bool, Counter(user_input),Counter(user_input_non_stemmed)]\n",
    "        if query == False:\n",
    "            break\n",
    "        search(file_names_list, query)\n",
    "        #time.sleep(10)\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    evaluate = input(\"Do you wish to run the evaluation program? (y/n): \").lower().strip()\n",
    "    if evaluate == 'y':\n",
    "        eval(file_names_list)\n",
    "    return\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MAP = 0.1683939966887202, Average R-Precision = 0.20571637719088887 @ 1.0\\nMAP = 0.1757442340606937, Average R-Precision = 0.20360779724255232 @ 0.8\\nMAP = 0.17996564162071682, Average R-Precision = 0.2040869887250585 @ 0.6\\nMAP = 0.17846120821370987, Average R-Precision = 0.20059048522855502 @0.4\\nMAP = 0.1783276612051629, Average R-Precision = 0.20059048522855502 @0.5\\nMAP = 0.1813458235983253, Average R-Precision = 0.2040869887250585 @0\\nMAP = 0.1809420606945624, Average R-Precision = 0.2040869887250585 @0.2\\nMAP = 0.1823457570881373, Average R-Precision = 0.20657842441487476 @0.1\\nMAP = 0.1828688572827038, Average R-Precision = 0.20947708484453706 @0.6\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"MAP = 0.1683939966887202, Average R-Precision = 0.20571637719088887 @ 1.0\n",
    "MAP = 0.1757442340606937, Average R-Precision = 0.20360779724255232 @ 0.8\n",
    "MAP = 0.17996564162071682, Average R-Precision = 0.2040869887250585 @ 0.6\n",
    "MAP = 0.17846120821370987, Average R-Precision = 0.20059048522855502 @0.4\n",
    "MAP = 0.1783276612051629, Average R-Precision = 0.20059048522855502 @0.5\n",
    "MAP = 0.1813458235983253, Average R-Precision = 0.2040869887250585 @0\n",
    "MAP = 0.1809420606945624, Average R-Precision = 0.2040869887250585 @0.2\n",
    "MAP = 0.1823457570881373, Average R-Precision = 0.20657842441487476 @0.1\n",
    "MAP = 0.1828688572827038, Average R-Precision = 0.20947708484453706 @0.6\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
